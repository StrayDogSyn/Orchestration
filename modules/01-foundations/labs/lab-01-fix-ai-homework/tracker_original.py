#!/usr/bin/env python3
"""
Job Application Tracker - Original (AI-Generated) Version
==========================================================

This is the "buggy" code that an AI enthusiast on your team generated.
It WORKS, but it doesn't SCALE.

Your job as an architect: Identify the performance issue and fix it.

INSTRUCTIONS:
1. Run this file: python tracker_original.py
2. Observe the performance output
3. Identify the O(n) bottleneck
4. Use an AI tool to help you refactor (with an architect's prompt!)

FILE STATUS: 
- Correctness: âœ… Works fine
- Performance: âŒ O(n) lookup - will become painfully slow at scale
- Architecture: âš ï¸ Needs review

Created for: Orchestration Course - Module 01, Lecture 01
"""

import time
from typing import List, Dict, Optional


class JobTracker:
    """Track job applications for justice-impacted job seekers.
    
    Generated by: ChatGPT (according to your teammate)
    Review status: PENDING ARCHITECT REVIEW
    
    KNOWN ISSUES (that your teammate didn't catch):
    - find_by_company() is O(n) - scans entire list every time
    - update_status() is O(n) - same problem
    - No indexing strategy for common queries
    
    WHY THIS MATTERS:
    - 100 applications: barely noticeable
    - 1,000 applications: starting to lag
    - 10,000 applications: unacceptably slow
    - 100,000 applications: unusable
    """
    
    def __init__(self):
        # ðŸš¨ ARCHITECT'S NOTE:
        # Using a list means EVERY lookup requires scanning ALL items.
        # This is fine for small datasets but becomes a bottleneck at scale.
        self.applications = []  # List of dicts
    
    def add_application(self, company: str, position: str, date: str) -> None:
        """Add a new job application.
        
        Time Complexity: O(1) - appending to a list is constant time
        This method is fine! No issues here.
        """
        self.applications.append({
            'company': company,
            'position': position,
            'date': date,
            'status': 'pending'
        })
    
    def find_by_company(self, company: str) -> List[Dict]:
        """Find all applications to a specific company.
        
        Time Complexity: O(n) where n = total applications
        
        ðŸš¨ THE PROBLEM:
        We scan EVERY application to find matches.
        If you have 10,000 applications and search for "Google",
        Python checks all 10,000 items even if only 3 match.
        
        THE FIX (that the architect would catch):
        Use a dictionary keyed by company name for O(1) lookup.
        Trade-off: Uses more memory, but lookups become instant.
        """
        results = []
        for app in self.applications:  # <-- O(n) scan here
            if app['company'] == company:
                results.append(app)
        return results
    
    def update_status(self, company: str, position: str, new_status: str) -> bool:
        """Update the status of a specific application.
        
        Time Complexity: O(n) - same problem as find_by_company
        
        ðŸš¨ THE PROBLEM:
        To update one application, we scan ALL applications.
        This gets worse as the dataset grows.
        """
        for app in self.applications:  # <-- O(n) scan here
            if app['company'] == company and app['position'] == position:
                app['status'] = new_status
                return True
        return False
    
    def get_all(self) -> List[Dict]:
        """Return all applications.
        
        Time Complexity: O(1) - just returns the reference
        This method is fine!
        """
        return self.applications
    
    def count(self) -> int:
        """Return total number of applications."""
        return len(self.applications)


# =============================================================================
# PERFORMANCE TESTING
# =============================================================================

def run_performance_test(num_applications: int = 1000) -> Dict:
    """
    Run a performance test on the JobTracker.
    
    This demonstrates the O(n) problem in action.
    """
    print(f"\n{'='*60}")
    print(f"ðŸ“Š PERFORMANCE TEST: {num_applications:,} applications")
    print(f"{'='*60}")
    
    tracker = JobTracker()
    
    # Phase 1: Add applications
    print("\nâ±ï¸  Phase 1: Adding applications...")
    start = time.time()
    
    for i in range(num_applications):
        # Distribute across 100 companies
        company = f"Company_{i % 100}"
        position = f"Position_{i}"
        date = "2025-01-01"
        tracker.add_application(company, position, date)
    
    add_time = time.time() - start
    print(f"   Added {num_applications:,} applications in {add_time:.4f} seconds")
    print(f"   Time per add: {(add_time/num_applications)*1000:.4f} ms")
    
    # Phase 2: Lookup performance
    print("\nðŸ” Phase 2: Testing lookup performance...")
    
    # Test single lookup
    start = time.time()
    results = tracker.find_by_company("Company_42")
    lookup_time = time.time() - start
    
    print(f"   Single lookup: {lookup_time*1000:.4f} ms")
    print(f"   Found {len(results)} applications for Company_42")
    
    # Test multiple lookups
    print("\nðŸ”„ Phase 3: Testing 100 consecutive lookups...")
    start = time.time()
    
    for i in range(100):
        company = f"Company_{i}"
        tracker.find_by_company(company)
    
    batch_time = time.time() - start
    print(f"   100 lookups completed in {batch_time:.4f} seconds")
    print(f"   Average per lookup: {(batch_time/100)*1000:.4f} ms")
    
    # Analysis
    print(f"\nðŸ“ˆ ANALYSIS:")
    print(f"   Total applications: {tracker.count():,}")
    print(f"   Lookup complexity: O(n) = O({tracker.count():,})")
    print(f"   ")
    print(f"   ðŸš¨ THE PROBLEM:")
    print(f"   Each lookup scans ALL {tracker.count():,} applications.")
    print(f"   If we had 100,000 applications, each lookup would scan 100,000 items!")
    print(f"   ")
    print(f"   ðŸ’¡ THE FIX:")
    print(f"   Use a dictionary indexed by company name.")
    print(f"   Lookups become O(1) - instant regardless of dataset size.")
    
    return {
        "num_applications": num_applications,
        "add_time": add_time,
        "lookup_time": lookup_time,
        "batch_time": batch_time
    }


def compare_at_scale():
    """
    Compare performance at different scales to show the O(n) problem.
    """
    print("\n" + "="*60)
    print("ðŸ”¬ SCALE COMPARISON TEST")
    print("="*60)
    print("\nThis shows how O(n) lookup gets worse as data grows.\n")
    
    scales = [100, 1000, 5000, 10000]
    results = []
    
    for n in scales:
        tracker = JobTracker()
        
        # Add n applications
        for i in range(n):
            tracker.add_application(f"Company_{i % 100}", f"Pos_{i}", "2025-01-01")
        
        # Time a single lookup
        start = time.time()
        tracker.find_by_company("Company_42")
        lookup_time = (time.time() - start) * 1000  # Convert to ms
        
        results.append((n, lookup_time))
        print(f"   {n:>6,} applications â†’ lookup time: {lookup_time:.4f} ms")
    
    print("\nðŸ“Š OBSERVATION:")
    print("   Notice how lookup time grows with dataset size.")
    print("   This is O(n) in action - linear growth.")
    print("   ")
    print("   With a dictionary (O(1)), lookup time would be ~constant")
    print("   regardless of whether we have 100 or 100,000 applications.")


# =============================================================================
# MAIN EXECUTION
# =============================================================================

if __name__ == "__main__":
    print("\n" + "ðŸŽ¯ "*20)
    print("JOB TRACKER PERFORMANCE ANALYSIS")
    print("Orchestration Course - Module 01, Lab 01")
    print("ðŸŽ¯ "*20)
    
    # Basic functionality test
    print("\n" + "="*60)
    print("âœ… FUNCTIONALITY TEST")
    print("="*60)
    
    tracker = JobTracker()
    tracker.add_application("Google", "Software Engineer", "2025-01-15")
    tracker.add_application("Microsoft", "Data Scientist", "2025-01-16")
    tracker.add_application("Google", "Product Manager", "2025-01-17")
    
    print(f"\n   Added 3 applications")
    print(f"   Total count: {tracker.count()}")
    print(f"   Google applications: {tracker.find_by_company('Google')}")
    print(f"   âœ… Basic functionality: WORKING")
    
    # Performance tests
    run_performance_test(1000)
    compare_at_scale()
    
    print("\n" + "="*60)
    print("ðŸŽ“ YOUR TASK AS AN ARCHITECT:")
    print("="*60)
    print("""
    1. The code WORKS. That's not the problem.
    
    2. The problem is SCALABILITY:
       - find_by_company() is O(n)
       - Every lookup scans ALL applications
       - This gets worse as data grows
    
    3. Your job:
       a) Understand WHY this is O(n)
       b) Write an architect's prompt to fix it
       c) Get AI to refactor with explanations
       d) Verify the fix improves performance
    
    Go to starter.ipynb to complete the lab!
    """)
